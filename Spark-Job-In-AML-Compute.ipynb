{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure SDK version: 1.0.76\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cesardl-automl-northcentralus-ws\n",
      "automlpmdemo\n",
      "northcentralus\n",
      "102a16c3-37d3-48a8-9237-4c9b1e8e80e0\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        #print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
    "        \n",
    "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
    "curated_environment = Environment.get(workspace=ws, name=\"AzureML-PySpark-MmlSpark-0.15\")\n",
    "\n",
    "# Save curated environment definition to folder (Two files, one for conda_dependencies.yml and another file for azureml_environment.json)\n",
    "curated_environment.save_to_directory(path=\"./curated_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base Environment from Conda specification\n",
    "\n",
    "from azureml.core import Environment\n",
    "\n",
    "# spark_environment = Environment(name=\"Custom-AzureML-PySpark-Environment\")\n",
    "spark_environment = Environment.from_conda_specification(name=\"Custom-AzureML-PySpark-Environment\", file_path=\"./curated_environment_definition/conda_dependencies.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ContainerRegistry\n",
    "\n",
    "# Set base Docker Image\n",
    "spark_environment.docker.enabled = True\n",
    "\n",
    "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
    "spark_environment.docker.base_image=\"mcr.microsoft.com/mmlspark/release\" \n",
    "container_registry = ContainerRegistry()\n",
    "container_registry.address = \"mcr.microsoft.com\"\n",
    "# container_registry.username = \"\"   # Use username if using a private Docker Registry like ACR\n",
    "# container_registry.password = \"\"   # Use password if using a private Docker Registry like ACR\n",
    "spark_environment.docker.base_image_registry=container_registry\n",
    "\n",
    "spark_environment.save_to_directory(path=\"./spark_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Name Custom-AzureML-PySpark-Environment\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.0.81.1\n",
      "  - azureml-defaults==1.0.81\n",
      "  - azureml-telemetry==1.0.81.1\n",
      "  - azureml-train-restclients-hyperdrive==1.0.81\n",
      "  - azureml-train-core==1.0.81\n",
      "name: azureml_2d6f32a8b5b445b7627fd1ae36599989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register Environment\n",
    "\n",
    "spark_environment.register(ws)\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"Custom\"):\n",
    "        print(\"Environment Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Experiment\n",
    "\n",
    "from azureml.core import Experiment\n",
    "experiment_name = 'test-spark-job-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./project-submit-folder/iris.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create project directory and copy the training script into the project directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './project-submit-folder'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Copy the needed files\n",
    "shutil.copy('spark-job.py', project_folder)\n",
    "shutil.copy('iris.csv', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing training cluster.\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Connect or Create a Remote AML compute cluster\n",
    "# Define remote compute target to use\n",
    "# Further docs on Remote Compute Target: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-remote\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cesardl-cpu-clus\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster\n",
    "     # Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 20)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ScriptRunConfig\n",
    "\n",
    "# Add training script to run config\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "\n",
    "script_runconfig = ScriptRunConfig(source_directory=project_folder, \n",
    "                            script=\"spark-job.py\"\n",
    "                            # arguments=[aml_dataset.as_named_input('attrition')]\n",
    "                           )\n",
    "\n",
    "# Attach compute target to run config\n",
    "script_runconfig.run_config.target = aml_remote_compute\n",
    "# runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "script_runconfig.run_config.environment = spark_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>test-spark-job-on-amlcompute</td><td>test-spark-job-on-amlcompute_1579837994_7290edde</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579837994_7290edde?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: test-spark-job-on-amlcompute,\n",
       "Id: test-spark-job-on-amlcompute_1579837994_7290edde,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(script_runconfig)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b952d3d07f6441cab819f230d75c07ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579837994_7290edde?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\", \"run_id\": \"test-spark-job-on-amlcompute_1579837994_7290edde\", \"run_properties\": {\"run_id\": \"test-spark-job-on-amlcompute_1579837994_7290edde\", \"created_utc\": \"2020-01-24T03:53:15.763286Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"ed96ab1e-d786-4a30-8ca4-208d5dea6c6e\", \"AzureML.DerivedImageName\": \"azureml/azureml_710c95ef0997b5fee10c1203f9c788eb\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-01-24T03:55:42.94458Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=9FNIBtcut3bADdzTGSjE31aC8n4itpAsb7x8L2KLMLw%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=ptsRjfAxwJjD11zVsNS2PMCSWynYmJFL3fL4TYudEaQ%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=QYWf2hf1ErXbWObi3CUqfjRbMjqhwj6Ee6j1soO6d%2Fs%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=lKwEbK82Vy4ffLilaMqMUvL0hYsyMT0FQW4U5CTrbOA%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"azureml-logs/process_info.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=o7GI83LnMGFaPUbpC3%2Fa62E5F5y51a0h9%2B07TIh49CE%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"azureml-logs/process_status.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=KdS5MAh4GVFC8JRKTaIpICtEmMt68HQupjZWXDG5Lnc%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"logs/azureml/395_azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/logs/azureml/395_azureml.log?sv=2019-02-02&sr=b&sig=YKkE7OnS%2BTDDMT%2FphvbMBZ3%2BzzXdlEMVSGwybBPbOEM%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\", \"logs/azureml/azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579837994_7290edde/logs/azureml/azureml.log?sv=2019-02-02&sr=b&sig=1atvSbpKhzzD2XSF8ZY05FEVUUuxuoSumeajlURCz8s%3D&st=2020-01-24T03%3A45%3A50Z&se=2020-01-24T11%3A55%3A50Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"logs/azureml/395_azureml.log\"]], \"run_duration\": \"0:02:27\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020-01-24 03:55:17,981|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2020-01-24 03:55:17,982|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2020-01-24 03:55:17,988|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2020-01-24 03:55:17,988|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-01-24 03:55:18,230|azureml._base_sdk_common.user_agent|DEBUG|Fetching client info from /root/.azureml/clientinfo.json\\n2020-01-24 03:55:18,231|azureml._base_sdk_common.user_agent|DEBUG|Error loading client info: [Errno 2] No such file or directory: '/root/.azureml/clientinfo.json'\\n2020-01-24 03:55:18,668|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-01-24 03:55:18,669|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-01-24 03:55:18,669|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-01-24 03:55:18,669|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-01-24 03:55:18,669|azureml.core.run|DEBUG|Adding new factory <function HyperDriveRun._from_run_dto at 0x7f4f2017fe18> for run source hyperdrive\\n2020-01-24 03:55:18,675|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f4f202527b8> for run source azureml.scriptrun\\n2020-01-24 03:55:18,676|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-01-24 03:55:18,684|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 03:55:18,685|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-01-24 03:55:18,685|azureml.core.authentication|DEBUG|Time to expire 1814276.314196 seconds\\n2020-01-24 03:55:18,686|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,686|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,686|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,687|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,687|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,687|azureml._base_sdk_common.service_discovery|DEBUG|Constructing mms service url in from history url environment variable None, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,687|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,687|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,688|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,866|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 03:55:18,873|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 03:55:18,883|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 03:55:18,890|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 03:55:18,897|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 03:55:18,904|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 03:55:18,905|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-01-24 03:55:18,905|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-01-24 03:55:18,906|msrest.http_logger|DEBUG|Request URL: 'https://northcentralus.experiments.azureml.net/history/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579837994_7290edde'\\n2020-01-24 03:55:18,907|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-01-24 03:55:18,907|msrest.http_logger|DEBUG|Request headers:\\n2020-01-24 03:55:18,907|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-01-24 03:55:18,907|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-01-24 03:55:18,907|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '6b23d5b7-16e5-44bb-b4ec-8fefb6e35ce1'\\n2020-01-24 03:55:18,907|msrest.http_logger|DEBUG|    'request-id': '6b23d5b7-16e5-44bb-b4ec-8fefb6e35ce1'\\n2020-01-24 03:55:18,908|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.10 azureml._restclient/core.1.0.81'\\n2020-01-24 03:55:18,908|msrest.http_logger|DEBUG|Request body:\\n2020-01-24 03:55:18,908|msrest.http_logger|DEBUG|None\\n2020-01-24 03:55:18,908|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-01-24 03:55:18,908|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-01-24 03:55:18,908|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-01-24 03:55:18,908|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-01-24 03:55:19,031|msrest.http_logger|DEBUG|Response status: 200\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|Response headers:\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|    'Date': 'Fri, 24 Jan 2020 03:55:19 GMT'\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-01-24 03:55:19,032|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-01-24 03:55:19,033|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '6b23d5b7-16e5-44bb-b4ec-8fefb6e35ce1'\\n2020-01-24 03:55:19,033|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-01-24 03:55:19,033|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-01-24 03:55:19,033|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-01-24 03:55:19,033|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-01-24 03:55:19,033|msrest.http_logger|DEBUG|Response content:\\n2020-01-24 03:55:19,034|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 3,\\n  \\\"rootRunId\\\": \\\"test-spark-job-on-amlcompute_1579837994_7290edde\\\",\\n  \\\"experimentId\\\": \\\"e14ee2a8-e31b-4059-a914-dd57fbfc2403\\\",\\n  \\\"createdUtc\\\": \\\"2020-01-24T03:53:15.7632866+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"6f9fb7d1-417b-46ae-9591-e9a89dfe7142\\\",\\n    \\\"userPuId\\\": \\\"10033FFF801BF74F\\\",\\n    \\\"userIdp\\\": null,\\n    \\\"userAltSecId\\\": null,\\n    \\\"userIss\\\": \\\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\\",\\n    \\\"userTenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\",\\n    \\\"userName\\\": \\\"Cesar De la Torre Llorente\\\"\\n  },\\n  \\\"userId\\\": \\\"6f9fb7d1-417b-46ae-9591-e9a89dfe7142\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 6,\\n  \\\"runUuid\\\": \\\"885a8377-f8e8-4b16-ac12-64a0b82e25af\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"885a8377-f8e8-4b16-ac12-64a0b82e25af\\\",\\n  \\\"runId\\\": \\\"test-spark-job-on-amlcompute_1579837994_7290edde\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-01-24T03:53:47.2372034+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.test-spark-job-on-amlcompute_1579837994_7290edde\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"ed96ab1e-d786-4a30-8ca4-208d5dea6c6e\\\",\\n    \\\"AzureML.DerivedImageName\\\": \\\"azureml/azureml_710c95ef0997b5fee10c1203f9c788eb\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"spark-job.py\\\",\\n  \\\"target\\\": \\\"cesardl-cpu-clus\\\",\\n  \\\"tags\\\": {},\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://northcentralus.experiments.azureml.net/execution/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runId/test-spark-job-on-amlcompute_1579837994_7290edde/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://northcentralus.experiments.azureml.net/execution/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runId/test-spark-job-on-amlcompute_1579837994_7290edde/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-01-24 03:55:19,041|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-01-24 03:55:19,041|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': 'ed96ab1e-d786-4a30-8ca4-208d5dea6c6e', 'AzureML.DerivedImageName': 'azureml/azureml_710c95ef0997b5fee10c1203f9c788eb', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-01-24 03:55:19,042|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-01-24 03:55:19,042|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2020-01-24 03:55:19,042|azureml.WorkerPool|DEBUG|[START]\\n2020-01-24 03:55:19,042|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-01-24 03:55:19,043|azureml.RunStatusContext|DEBUG|[START]\\n2020-01-24 03:55:19,043|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-01-24 03:55:19,043|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-01-24 03:55:19,043|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-01-24 03:55:19,043|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde\\n2020-01-24 03:55:19,043|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-01-24 03:55:19,044|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde\\n2020-01-24 03:55:19,851|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-01-24 03:55:19,851|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde\\n2020-01-24 03:55:19,851|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde to /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde\\n2020-01-24 03:55:19,852|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde\\n2020-01-24 03:55:19,852|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-01-24 03:55:19,852|azureml.WorkingDirectoryCM|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4f1f84d788>\\n2020-01-24 03:55:19,852|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-01-24 03:55:19,852|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde|INFO|fail is not setting status for submitted runs.\\n2020-01-24 03:55:19,852|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-01-24 03:55:19,853|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-01-24 03:55:19,853|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-01-24 03:55:19,853|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-01-24 03:55:19,854|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-01-24 03:55:19,854|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-01-24 03:55:19,854|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-01-24 03:55:19,854|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-01-24 03:55:19,854|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-01-24 03:55:19,854|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-01-24 03:55:19,855|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.RunClient.post-async:False|DEBUG|[START]\\n2020-01-24 03:55:19,855|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-01-24 03:55:19,856|msrest.http_logger|DEBUG|Request URL: 'https://northcentralus.experiments.azureml.net/history/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579837994_7290edde/events'\\n2020-01-24 03:55:19,856|msrest.http_logger|DEBUG|Request method: 'POST'\\n2020-01-24 03:55:19,856|msrest.http_logger|DEBUG|Request headers:\\n2020-01-24 03:55:19,856|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-01-24 03:55:19,856|msrest.http_logger|DEBUG|    'Content-Type': 'application/json-patch+json; charset=utf-8'\\n2020-01-24 03:55:19,856|msrest.http_logger|DEBUG|    'x-ms-caller-name': 'RunHistoryFacade'\\n2020-01-24 03:55:19,857|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'af2dd3e4-e696-4bbd-b8dc-146a054feae2'\\n2020-01-24 03:55:19,857|msrest.http_logger|DEBUG|    'request-id': 'af2dd3e4-e696-4bbd-b8dc-146a054feae2'\\n2020-01-24 03:55:19,857|msrest.http_logger|DEBUG|    'Content-Length': '1354'\\n2020-01-24 03:55:19,857|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.10 azureml._restclient/core.1.0.81'\\n2020-01-24 03:55:19,857|msrest.http_logger|DEBUG|Request body:\\n2020-01-24 03:55:19,857|msrest.http_logger|DEBUG|{\\\"timestamp\\\": \\\"2020-01-24T03:55:19.854966Z\\\", \\\"name\\\": \\\"Microsoft.MachineLearning.Run.Error\\\", \\\"data\\\": {\\\"RunId\\\": \\\"test-spark-job-on-amlcompute_1579837994_7290edde\\\", \\\"ErrorResponse\\\": {\\\"error\\\": {\\\"code\\\": \\\"UserError\\\", \\\"message\\\": \\\"User program failed with ModuleNotFoundError: No module named 'pyspark'\\\", \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\", \\\"debugInfo\\\": {\\\"type\\\": \\\"ModuleNotFoundError\\\", \\\"message\\\": \\\"No module named 'pyspark'\\\", \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579837994_7290edde/azureml-setup/context_manager_injector.py\\\\\\\", line 119, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_2d6f32a8b5b445b7627fd1ae36599989/lib/python3.6/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_2d6f32a8b5b445b7627fd1ae36599989/lib/python3.6/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_2d6f32a8b5b445b7627fd1ae36599989/lib/python3.6/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"spark-job.py\\\\\\\", line 3, in <module>\\\\n    import pyspark\\\\n\\\"}}}}}\\n2020-01-24 03:55:19,858|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-01-24 03:55:19,858|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-01-24 03:55:19,858|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-01-24 03:55:19,858|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-01-24 03:55:20,755|msrest.http_logger|DEBUG|Response status: 200\\n2020-01-24 03:55:20,755|msrest.http_logger|DEBUG|Response headers:\\n2020-01-24 03:55:20,756|msrest.http_logger|DEBUG|    'Date': 'Fri, 24 Jan 2020 03:55:20 GMT'\\n2020-01-24 03:55:20,756|msrest.http_logger|DEBUG|    'Content-Length': '0'\\n2020-01-24 03:55:20,756|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-01-24 03:55:20,756|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-01-24 03:55:20,756|msrest.http_logger|DEBUG|    'x-ms-client-request-id': 'af2dd3e4-e696-4bbd-b8dc-146a054feae2'\\n2020-01-24 03:55:20,756|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-01-24 03:55:20,757|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-01-24 03:55:20,757|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-01-24 03:55:20,757|msrest.http_logger|DEBUG|Response content:\\n2020-01-24 03:55:20,757|msrest.http_logger|DEBUG|\\n2020-01-24 03:55:20,759|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.RunClient.post-async:False|DEBUG|[STOP]\\n2020-01-24 03:55:20,759|azureml.RunStatusContext|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4f1f84d788>\\n2020-01-24 03:55:20,759|azureml.RunStatusContext|DEBUG|[STOP]\\n2020-01-24 03:55:20,760|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-01-24 03:55:20,760|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-01-24 03:55:20,760|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-01-24 03:55:20,760|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-01-24 03:55:20,760|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-01-24 03:55:20,760|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-01-24 03:55:20,761|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579837994_7290edde.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-01-24 03:55:20,761|azureml.SendRunKillSignal|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4f1f84d788>\\n2020-01-24 03:55:20,761|azureml.SendRunKillSignal|DEBUG|[STOP]\\n2020-01-24 03:55:20,761|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[START]\\n2020-01-24 03:55:20,761|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[STOP]\\n2020-01-24 03:55:20,762|azureml.WorkerPool|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4f1f84d788>\\n2020-01-24 03:55:20,762|azureml.WorkerPool|DEBUG|[STOP]\\n\\nError occurred: User program failed with ModuleNotFoundError: No module named 'pyspark'\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"NOTSET\", \"sdk_version\": \"1.0.76\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monitor run\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
