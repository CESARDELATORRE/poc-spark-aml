{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure SDK version: 1.0.76\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cesardl-automl-northcentralus-ws\n",
      "automlpmdemo\n",
      "northcentralus\n",
      "102a16c3-37d3-48a8-9237-4c9b1e8e80e0\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        #print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
    "        \n",
    "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
    "curated_environment = Environment.get(workspace=ws, name=\"AzureML-PySpark-MmlSpark-0.15\")\n",
    "\n",
    "# Save curated environment definition to folder (Two files, one for conda_dependencies.yml and another file for azureml_environment.json)\n",
    "curated_environment.save_to_directory(path=\"./curated_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy based on curated environment\n",
    "spark_environment = curated_environment\n",
    "spark_environment.name = \"Custom-AzureML-PySpark-Environment\"\n",
    "\n",
    "# Create base Environment from Conda specification\n",
    "# spark_environment = Environment.from_conda_specification(name=\"Custom-AzureML-PySpark-Environment\", file_path=\"./curated_environment_definition/conda_dependencies.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ContainerRegistry\n",
    "\n",
    "# Set base Docker Image\n",
    "spark_environment.docker.enabled = True\n",
    "\n",
    "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
    "spark_environment.docker.base_image=\"mcr.microsoft.com/mmlspark/release\" \n",
    "container_registry = ContainerRegistry()\n",
    "container_registry.address = \"mcr.microsoft.com\"\n",
    "# container_registry.username = \"\"   # Use username if using a private Docker Registry like ACR\n",
    "# container_registry.password = \"\"   # Use password if using a private Docker Registry like ACR\n",
    "spark_environment.docker.base_image_registry=container_registry\n",
    "\n",
    "spark_environment.save_to_directory(path=\"./spark_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Name Custom-AzureML-PySpark-Environment\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.0.81.1\n",
      "  - azureml-defaults==1.0.81\n",
      "  - azureml-telemetry==1.0.81.1\n",
      "  - azureml-train-restclients-hyperdrive==1.0.81\n",
      "  - azureml-train-core==1.0.81\n",
      "name: azureml_2d6f32a8b5b445b7627fd1ae36599989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register Environment\n",
    "\n",
    "spark_environment.register(ws)\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"Custom\"):\n",
    "        print(\"Environment Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Experiment\n",
    "\n",
    "from azureml.core import Experiment\n",
    "experiment_name = 'test-spark-job-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./project-submit-folder/iris.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create project directory and copy the training script into the project directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './project-submit-folder'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Copy the needed files\n",
    "shutil.copy('spark-job.py', project_folder)\n",
    "shutil.copy('iris.csv', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing training cluster.\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Connect or Create a Remote AML compute cluster\n",
    "# Define remote compute target to use\n",
    "# Further docs on Remote Compute Target: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-remote\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cesardl-cpu-clus\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster\n",
    "     # Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 20)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ScriptRunConfig\n",
    "\n",
    "# Add training script to run config\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "\n",
    "script_runconfig = ScriptRunConfig(source_directory=project_folder, \n",
    "                            script=\"spark-job.py\"\n",
    "                            # arguments=[aml_dataset.as_named_input('attrition')]\n",
    "                           )\n",
    "\n",
    "# Attach compute target to run config\n",
    "script_runconfig.run_config.target = aml_remote_compute\n",
    "# runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "script_runconfig.run_config.environment = spark_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>test-spark-job-on-amlcompute</td><td>test-spark-job-on-amlcompute_1579843336_dc573f2a</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579843336_dc573f2a?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: test-spark-job-on-amlcompute,\n",
       "Id: test-spark-job-on-amlcompute_1579843336_dc573f2a,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(script_runconfig)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9632ca5f1e64c53a92e470d205191e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579843336_dc573f2a?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\", \"run_id\": \"test-spark-job-on-amlcompute_1579843336_dc573f2a\", \"run_properties\": {\"run_id\": \"test-spark-job-on-amlcompute_1579843336_dc573f2a\", \"created_utc\": \"2020-01-24T05:22:18.822529Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"db2c8275-910c-493c-9d20-4d1cb7f74d1f\", \"azureml.git.repository_uri\": \"https://github.com/CESARDELATORRE/poc-spark-aml.git\", \"mlflow.source.git.repoURL\": \"https://github.com/CESARDELATORRE/poc-spark-aml.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"8800fa21a19835620c5cb5e3b71ce7361502b218\", \"mlflow.source.git.commit\": \"8800fa21a19835620c5cb5e3b71ce7361502b218\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_5791559675f375a8eab3c368318bea04\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":1}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-01-24T05:34:44.241047Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=xFqPLt3byMghdp8apR0lZjseu6sbVaK3VRgdHWBS1%2FY%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=DrakJ83uKKz%2BO4TwlEcqcGCl7%2FbR69tsC2O0wawB%2Bwo%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=%2B3g%2FoIXkU%2BGlPTp2WGX8uMzSQtLaxcjEcG%2Fxm6Gfnng%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=KV2GyOLtVZxLcwSw2BgTdnQ57yg3clOJbBDSWE2U9%2BY%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=zk0dt7sm8HUUdq4H0WHI4UfM4IhXrvoNvmlbzhRnoSY%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"azureml-logs/process_info.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=jzITPjobfNDxXneAOT0bGqwJkxD4qSHh1pLYytrl34Y%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"azureml-logs/process_status.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=7SW%2BmUXOFu5sw7%2BbEsTpfp4FDg6JbU%2FaVXUudHU%2FPzs%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"logs/azureml/397_azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/logs/azureml/397_azureml.log?sv=2019-02-02&sr=b&sig=TlQKgQjeaj5YS0ZGDkfOsdZqgmZgsAhUVzJ%2F36lH62Q%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\", \"logs/azureml/azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a/logs/azureml/azureml.log?sv=2019-02-02&sr=b&sig=ujcxhohgh460McReoVSwKq%2B3WK%2F%2F%2BpZRAFEM4fSoaFQ%3D&st=2020-01-24T05%3A24%3A59Z&se=2020-01-24T13%3A34%3A59Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/azureml.log\"], [\"azureml-logs/20_image_build_log.txt\"], [\"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"logs/azureml/397_azureml.log\"]], \"run_duration\": \"0:12:25\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020-01-24 05:34:19,585|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2020-01-24 05:34:19,586|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2020-01-24 05:34:19,591|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2020-01-24 05:34:19,592|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2020-01-24 05:34:19,844|azureml._base_sdk_common.user_agent|DEBUG|Fetching client info from /root/.azureml/clientinfo.json\\n2020-01-24 05:34:19,845|azureml._base_sdk_common.user_agent|DEBUG|Error loading client info: [Errno 2] No such file or directory: '/root/.azureml/clientinfo.json'\\n2020-01-24 05:34:20,279|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-01-24 05:34:20,279|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveRunConfig'>\\n2020-01-24 05:34:20,279|azureml.core._experiment_method|DEBUG|Trying to register submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-01-24 05:34:20,280|azureml.core._experiment_method|DEBUG|Registered submit_function search, on method <class 'azureml.train.hyperdrive.runconfig.HyperDriveConfig'>\\n2020-01-24 05:34:20,280|azureml.core.run|DEBUG|Adding new factory <function HyperDriveRun._from_run_dto at 0x7f4058dd8e18> for run source hyperdrive\\n2020-01-24 05:34:20,286|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f4058f267b8> for run source azureml.scriptrun\\n2020-01-24 05:34:20,287|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-01-24 05:34:20,295|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 05:34:20,296|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2020-01-24 05:34:20,297|azureml.core.authentication|DEBUG|Time to expire 1813677.702927 seconds\\n2020-01-24 05:34:20,297|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,298|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,298|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,298|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,298|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,298|azureml._base_sdk_common.service_discovery|DEBUG|Constructing mms service url in from history url environment variable None, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,299|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,299|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,299|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,352|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://northcentralus.experiments.azureml.net.\\n2020-01-24 05:34:20,358|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 05:34:20,371|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 05:34:20,378|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 05:34:20,385|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 05:34:20,393|msrest.universal_http.requests|DEBUG|Configuring retry: max_retries=3, backoff_factor=0.8, max_backoff=90\\n2020-01-24 05:34:20,393|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.RunClient.get-async:False|DEBUG|[START]\\n2020-01-24 05:34:20,394|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-01-24 05:34:20,394|msrest.http_logger|DEBUG|Request URL: 'https://northcentralus.experiments.azureml.net/history/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579843336_dc573f2a'\\n2020-01-24 05:34:20,394|msrest.http_logger|DEBUG|Request method: 'GET'\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|Request headers:\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '558f058f-853b-46dd-9705-80d19a1ac00a'\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|    'request-id': '558f058f-853b-46dd-9705-80d19a1ac00a'\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.10 azureml._restclient/core.1.0.81'\\n2020-01-24 05:34:20,395|msrest.http_logger|DEBUG|Request body:\\n2020-01-24 05:34:20,396|msrest.http_logger|DEBUG|None\\n2020-01-24 05:34:20,396|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-01-24 05:34:20,396|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-01-24 05:34:20,396|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-01-24 05:34:20,396|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-01-24 05:34:21,246|msrest.http_logger|DEBUG|Response status: 200\\n2020-01-24 05:34:21,246|msrest.http_logger|DEBUG|Response headers:\\n2020-01-24 05:34:21,247|msrest.http_logger|DEBUG|    'Date': 'Fri, 24 Jan 2020 05:34:21 GMT'\\n2020-01-24 05:34:21,247|msrest.http_logger|DEBUG|    'Content-Type': 'application/json; charset=utf-8'\\n2020-01-24 05:34:21,247|msrest.http_logger|DEBUG|    'Transfer-Encoding': 'chunked'\\n2020-01-24 05:34:21,247|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-01-24 05:34:21,247|msrest.http_logger|DEBUG|    'Vary': 'Accept-Encoding'\\n2020-01-24 05:34:21,247|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-01-24 05:34:21,248|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '558f058f-853b-46dd-9705-80d19a1ac00a'\\n2020-01-24 05:34:21,248|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-01-24 05:34:21,248|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-01-24 05:34:21,248|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-01-24 05:34:21,248|msrest.http_logger|DEBUG|    'Content-Encoding': 'gzip'\\n2020-01-24 05:34:21,249|msrest.http_logger|DEBUG|Response content:\\n2020-01-24 05:34:21,249|msrest.http_logger|DEBUG|{\\n  \\\"runNumber\\\": 5,\\n  \\\"rootRunId\\\": \\\"test-spark-job-on-amlcompute_1579843336_dc573f2a\\\",\\n  \\\"experimentId\\\": \\\"e14ee2a8-e31b-4059-a914-dd57fbfc2403\\\",\\n  \\\"createdUtc\\\": \\\"2020-01-24T05:22:18.8225295+00:00\\\",\\n  \\\"createdBy\\\": {\\n    \\\"userObjectId\\\": \\\"6f9fb7d1-417b-46ae-9591-e9a89dfe7142\\\",\\n    \\\"userPuId\\\": \\\"10033FFF801BF74F\\\",\\n    \\\"userIdp\\\": null,\\n    \\\"userAltSecId\\\": null,\\n    \\\"userIss\\\": \\\"https://sts.windows.net/72f988bf-86f1-41af-91ab-2d7cd011db47/\\\",\\n    \\\"userTenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\",\\n    \\\"userName\\\": \\\"Cesar De la Torre Llorente\\\"\\n  },\\n  \\\"userId\\\": \\\"6f9fb7d1-417b-46ae-9591-e9a89dfe7142\\\",\\n  \\\"token\\\": null,\\n  \\\"tokenExpiryTimeUtc\\\": null,\\n  \\\"error\\\": null,\\n  \\\"warnings\\\": null,\\n  \\\"revision\\\": 9,\\n  \\\"runUuid\\\": \\\"7f6ac2ca-1129-4771-b04a-d290b0c53525\\\",\\n  \\\"parentRunUuid\\\": null,\\n  \\\"rootRunUuid\\\": \\\"7f6ac2ca-1129-4771-b04a-d290b0c53525\\\",\\n  \\\"runId\\\": \\\"test-spark-job-on-amlcompute_1579843336_dc573f2a\\\",\\n  \\\"parentRunId\\\": null,\\n  \\\"status\\\": \\\"Running\\\",\\n  \\\"startTimeUtc\\\": \\\"2020-01-24T05:33:50.2627681+00:00\\\",\\n  \\\"endTimeUtc\\\": null,\\n  \\\"heartbeatEnabled\\\": false,\\n  \\\"options\\\": {\\n    \\\"generateDataContainerIdIfNotSpecified\\\": true\\n  },\\n  \\\"name\\\": null,\\n  \\\"dataContainerId\\\": \\\"dcid.test-spark-job-on-amlcompute_1579843336_dc573f2a\\\",\\n  \\\"description\\\": null,\\n  \\\"hidden\\\": false,\\n  \\\"runType\\\": \\\"azureml.scriptrun\\\",\\n  \\\"properties\\\": {\\n    \\\"_azureml.ComputeTargetType\\\": \\\"amlcompute\\\",\\n    \\\"ContentSnapshotId\\\": \\\"db2c8275-910c-493c-9d20-4d1cb7f74d1f\\\",\\n    \\\"azureml.git.repository_uri\\\": \\\"https://github.com/CESARDELATORRE/poc-spark-aml.git\\\",\\n    \\\"mlflow.source.git.repoURL\\\": \\\"https://github.com/CESARDELATORRE/poc-spark-aml.git\\\",\\n    \\\"azureml.git.branch\\\": \\\"master\\\",\\n    \\\"mlflow.source.git.branch\\\": \\\"master\\\",\\n    \\\"azureml.git.commit\\\": \\\"8800fa21a19835620c5cb5e3b71ce7361502b218\\\",\\n    \\\"mlflow.source.git.commit\\\": \\\"8800fa21a19835620c5cb5e3b71ce7361502b218\\\",\\n    \\\"azureml.git.dirty\\\": \\\"True\\\",\\n    \\\"AzureML.DerivedImageName\\\": \\\"azureml/azureml_5791559675f375a8eab3c368318bea04\\\",\\n    \\\"ProcessInfoFile\\\": \\\"azureml-logs/process_info.json\\\",\\n    \\\"ProcessStatusFile\\\": \\\"azureml-logs/process_status.json\\\"\\n  },\\n  \\\"scriptName\\\": \\\"spark-job.py\\\",\\n  \\\"target\\\": \\\"cesardl-cpu-clus\\\",\\n  \\\"tags\\\": {\\n    \\\"_aml_system_ComputeTargetStatus\\\": \\\"{\\\\\\\"AllocationState\\\\\\\":\\\\\\\"steady\\\\\\\",\\\\\\\"PreparingNodeCount\\\\\\\":0,\\\\\\\"RunningNodeCount\\\\\\\":0,\\\\\\\"CurrentNodeCount\\\\\\\":1}\\\"\\n  },\\n  \\\"inputDatasets\\\": [],\\n  \\\"runDefinition\\\": null,\\n  \\\"createdFrom\\\": null,\\n  \\\"cancelUri\\\": \\\"https://northcentralus.experiments.azureml.net/execution/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runId/test-spark-job-on-amlcompute_1579843336_dc573f2a/cancel\\\",\\n  \\\"completeUri\\\": null,\\n  \\\"diagnosticsUri\\\": \\\"https://northcentralus.experiments.azureml.net/execution/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runId/test-spark-job-on-amlcompute_1579843336_dc573f2a/diagnostics\\\",\\n  \\\"computeRequest\\\": {\\n    \\\"nodeCount\\\": 1\\n  },\\n  \\\"retainForLifetimeOfWorkspace\\\": false,\\n  \\\"queueingInfo\\\": null\\n}\\n2020-01-24 05:34:21,256|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.RunClient.get-async:False|DEBUG|[STOP]\\n2020-01-24 05:34:21,257|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': 'db2c8275-910c-493c-9d20-4d1cb7f74d1f', 'azureml.git.repository_uri': 'https://github.com/CESARDELATORRE/poc-spark-aml.git', 'mlflow.source.git.repoURL': 'https://github.com/CESARDELATORRE/poc-spark-aml.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '8800fa21a19835620c5cb5e3b71ce7361502b218', 'mlflow.source.git.commit': '8800fa21a19835620c5cb5e3b71ce7361502b218', 'azureml.git.dirty': 'True', 'AzureML.DerivedImageName': 'azureml/azureml_5791559675f375a8eab3c368318bea04', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2020-01-24 05:34:21,257|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2020-01-24 05:34:21,258|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2020-01-24 05:34:21,258|azureml.WorkerPool|DEBUG|[START]\\n2020-01-24 05:34:21,258|azureml.SendRunKillSignal|DEBUG|[START]\\n2020-01-24 05:34:21,259|azureml.RunStatusContext|DEBUG|[START]\\n2020-01-24 05:34:21,259|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunContextManager.RunStatusContext|DEBUG|[START]\\n2020-01-24 05:34:21,259|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2020-01-24 05:34:21,259|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2020-01-24 05:34:21,259|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a\\n2020-01-24 05:34:21,259|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-01-24 05:34:21,260|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a\\n2020-01-24 05:34:21,719|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2020-01-24 05:34:21,719|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a\\n2020-01-24 05:34:21,719|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a to /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a\\n2020-01-24 05:34:21,720|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a\\n2020-01-24 05:34:21,720|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2020-01-24 05:34:21,720|azureml.WorkingDirectoryCM|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4058526c48>\\n2020-01-24 05:34:21,720|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2020-01-24 05:34:21,721|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a|INFO|fail is not setting status for submitted runs.\\n2020-01-24 05:34:21,721|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-01-24 05:34:21,721|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2020-01-24 05:34:21,721|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2020-01-24 05:34:21,723|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2020-01-24 05:34:21,723|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-01-24 05:34:21,723|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2020-01-24 05:34:21,724|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2020-01-24 05:34:21,724|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-01-24 05:34:21,724|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-01-24 05:34:21,724|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-01-24 05:34:21,725|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.RunClient.post-async:False|DEBUG|[START]\\n2020-01-24 05:34:21,726|msrest.service_client|DEBUG|Accept header absent and forced to application/json\\n2020-01-24 05:34:21,726|msrest.http_logger|DEBUG|Request URL: 'https://northcentralus.experiments.azureml.net/history/v1.0/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourceGroups/automlpmdemo/providers/Microsoft.MachineLearningServices/workspaces/cesardl-automl-northcentralus-ws/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579843336_dc573f2a/events'\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|Request method: 'POST'\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|Request headers:\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|    'Accept': 'application/json'\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|    'Content-Type': 'application/json-patch+json; charset=utf-8'\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|    'x-ms-caller-name': 'RunHistoryFacade'\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '5bd28eb4-d300-4c4d-b00c-5e5a6e8c4a2c'\\n2020-01-24 05:34:21,727|msrest.http_logger|DEBUG|    'request-id': '5bd28eb4-d300-4c4d-b00c-5e5a6e8c4a2c'\\n2020-01-24 05:34:21,728|msrest.http_logger|DEBUG|    'Content-Length': '1354'\\n2020-01-24 05:34:21,728|msrest.http_logger|DEBUG|    'User-Agent': 'python/3.6.2 (Linux-4.15.0-1057-azure-x86_64-with-debian-stretch-sid) msrest/0.6.10 azureml._restclient/core.1.0.81'\\n2020-01-24 05:34:21,728|msrest.http_logger|DEBUG|Request body:\\n2020-01-24 05:34:21,728|msrest.http_logger|DEBUG|{\\\"timestamp\\\": \\\"2020-01-24T05:34:21.724844Z\\\", \\\"name\\\": \\\"Microsoft.MachineLearning.Run.Error\\\", \\\"data\\\": {\\\"RunId\\\": \\\"test-spark-job-on-amlcompute_1579843336_dc573f2a\\\", \\\"ErrorResponse\\\": {\\\"error\\\": {\\\"code\\\": \\\"UserError\\\", \\\"message\\\": \\\"User program failed with ModuleNotFoundError: No module named 'pyspark'\\\", \\\"detailsUri\\\": \\\"https://aka.ms/azureml-known-errors\\\", \\\"debugInfo\\\": {\\\"type\\\": \\\"ModuleNotFoundError\\\", \\\"message\\\": \\\"No module named 'pyspark'\\\", \\\"stackTrace\\\": \\\"  File \\\\\\\"/mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579843336_dc573f2a/azureml-setup/context_manager_injector.py\\\\\\\", line 119, in execute_with_context\\\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\\\\\"__main__\\\\\\\")\\\\n  File \\\\\\\"/azureml-envs/azureml_2d6f32a8b5b445b7627fd1ae36599989/lib/python3.6/runpy.py\\\\\\\", line 263, in run_path\\\\n    pkg_name=pkg_name, script_name=fname)\\\\n  File \\\\\\\"/azureml-envs/azureml_2d6f32a8b5b445b7627fd1ae36599989/lib/python3.6/runpy.py\\\\\\\", line 96, in _run_module_code\\\\n    mod_name, mod_spec, pkg_name, script_name)\\\\n  File \\\\\\\"/azureml-envs/azureml_2d6f32a8b5b445b7627fd1ae36599989/lib/python3.6/runpy.py\\\\\\\", line 85, in _run_code\\\\n    exec(code, run_globals)\\\\n  File \\\\\\\"spark-job.py\\\\\\\", line 3, in <module>\\\\n    import pyspark\\\\n\\\"}}}}}\\n2020-01-24 05:34:21,728|msrest.universal_http|DEBUG|Configuring redirects: allow=True, max=30\\n2020-01-24 05:34:21,728|msrest.universal_http|DEBUG|Configuring request: timeout=100, verify=True, cert=None\\n2020-01-24 05:34:21,729|msrest.universal_http|DEBUG|Configuring proxies: ''\\n2020-01-24 05:34:21,729|msrest.universal_http|DEBUG|Evaluate proxies against ENV settings: True\\n2020-01-24 05:34:21,823|msrest.http_logger|DEBUG|Response status: 200\\n2020-01-24 05:34:21,824|msrest.http_logger|DEBUG|Response headers:\\n2020-01-24 05:34:21,824|msrest.http_logger|DEBUG|    'Date': 'Fri, 24 Jan 2020 05:34:21 GMT'\\n2020-01-24 05:34:21,824|msrest.http_logger|DEBUG|    'Content-Length': '0'\\n2020-01-24 05:34:21,824|msrest.http_logger|DEBUG|    'Connection': 'keep-alive'\\n2020-01-24 05:34:21,824|msrest.http_logger|DEBUG|    'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d'\\n2020-01-24 05:34:21,825|msrest.http_logger|DEBUG|    'x-ms-client-request-id': '5bd28eb4-d300-4c4d-b00c-5e5a6e8c4a2c'\\n2020-01-24 05:34:21,825|msrest.http_logger|DEBUG|    'x-ms-client-session-id': ''\\n2020-01-24 05:34:21,825|msrest.http_logger|DEBUG|    'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'\\n2020-01-24 05:34:21,825|msrest.http_logger|DEBUG|    'X-Content-Type-Options': 'nosniff'\\n2020-01-24 05:34:21,825|msrest.http_logger|DEBUG|Response content:\\n2020-01-24 05:34:21,825|msrest.http_logger|DEBUG|\\n2020-01-24 05:34:21,827|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.RunClient.post-async:False|DEBUG|[STOP]\\n2020-01-24 05:34:21,827|azureml.RunStatusContext|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4058526c48>\\n2020-01-24 05:34:21,827|azureml.RunStatusContext|DEBUG|[STOP]\\n2020-01-24 05:34:21,828|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2020-01-24 05:34:21,828|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2020-01-24 05:34:21,828|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300.0 is different from task queue timeout 120, using flush timeout\\n2020-01-24 05:34:21,829|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300.0 seconds on tasks: [].\\n2020-01-24 05:34:21,829|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2020-01-24 05:34:21,829|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2020-01-24 05:34:21,829|azureml._SubmittedRun#test-spark-job-on-amlcompute_1579843336_dc573f2a.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2020-01-24 05:34:21,829|azureml.SendRunKillSignal|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4058526c48>\\n2020-01-24 05:34:21,829|azureml.SendRunKillSignal|DEBUG|[STOP]\\n2020-01-24 05:34:21,830|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[START]\\n2020-01-24 05:34:21,830|azureml.HistoryTrackingWorkerPool.WorkerPoolShutdown|DEBUG|[STOP]\\n2020-01-24 05:34:21,830|azureml.WorkerPool|ERROR|<class 'ModuleNotFoundError'>: No module named 'pyspark'\\n<traceback object at 0x7f4058526c48>\\n2020-01-24 05:34:21,830|azureml.WorkerPool|DEBUG|[STOP]\\n\\nError occurred: User program failed with ModuleNotFoundError: No module named 'pyspark'\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"NOTSET\", \"sdk_version\": \"1.0.76\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monitor run\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
