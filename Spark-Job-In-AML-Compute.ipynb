{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure SDK version: 1.0.76\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cesardl-automl-northcentralus-ws\n",
      "automlpmdemo\n",
      "northcentralus\n",
      "102a16c3-37d3-48a8-9237-4c9b1e8e80e0\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        #print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
    "        \n",
    "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
    "curated_environment = Environment.get(workspace=ws, name=\"AzureML-PySpark-MmlSpark-0.15\")\n",
    "\n",
    "# Save curated environment definition to folder (Two files, one for conda_dependencies.yml and another file for azureml_environment.json)\n",
    "curated_environment.save_to_directory(path=\"./curated_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy based on curated environment\n",
    "spark_environment = curated_environment\n",
    "spark_environment.name = \"Custom-AzureML-PySpark-Environment\"\n",
    "\n",
    "# Create base Environment from Conda specification\n",
    "# spark_environment = Environment.from_conda_specification(name=\"Custom-AzureML-PySpark-Environment\", file_path=\"./curated_environment_definition/conda_dependencies.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ContainerRegistry\n",
    "\n",
    "# Set base Docker Image\n",
    "spark_environment.docker.enabled = True\n",
    "\n",
    "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
    "spark_environment.docker.base_image=\"mcr.microsoft.com/mmlspark/release\" \n",
    "container_registry = ContainerRegistry()\n",
    "container_registry.address = \"mcr.microsoft.com\"\n",
    "# container_registry.username = \"\"   # Use username if using a private Docker Registry like ACR\n",
    "# container_registry.password = \"\"   # Use password if using a private Docker Registry like ACR\n",
    "spark_environment.docker.base_image_registry=container_registry\n",
    "\n",
    "spark_environment.save_to_directory(path=\"./spark_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Name Custom-AzureML-PySpark-Environment\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.0.81.1\n",
      "  - azureml-defaults==1.0.81\n",
      "  - azureml-telemetry==1.0.81.1\n",
      "  - azureml-train-restclients-hyperdrive==1.0.81\n",
      "  - azureml-train-core==1.0.81\n",
      "name: azureml_2d6f32a8b5b445b7627fd1ae36599989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register Environment\n",
    "\n",
    "spark_environment.register(ws)\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"Custom\"):\n",
    "        print(\"Environment Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Experiment\n",
    "\n",
    "from azureml.core import Experiment\n",
    "experiment_name = 'test-spark-job-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./project-submit-folder/iris.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create project directory and copy the training script into the project directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './project-submit-folder'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Copy the needed files\n",
    "shutil.copy('spark-job.py', project_folder)\n",
    "shutil.copy('iris.csv', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing training cluster.\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Connect or Create a Remote AML compute cluster\n",
    "# Define remote compute target to use\n",
    "# Further docs on Remote Compute Target: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-remote\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cesardl-cpu-clus\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster\n",
    "     # Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 20)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ScriptRunConfig\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "## use pyspark framework\n",
    "# spark_run_config = RunConfiguration(framework=\"pyspark\")\n",
    "## Set compute target to the cluster\n",
    "# spark_run_config.target = aml_remote_compute.name\n",
    "\n",
    "# specify CondaDependencies object to ask system installing numpy\n",
    "cd = CondaDependencies()\n",
    "cd.add_conda_package('numpy')\n",
    "# spark_run_config.environment.python.conda_dependencies = cd\n",
    "\n",
    "script_runconfig = ScriptRunConfig(source_directory=project_folder, \n",
    "                            script=\"spark-job.py\",\n",
    "                            run_config = spark_run_config]\n",
    "                           )\n",
    "\n",
    "# Attach compute target to run config\n",
    "script_runconfig.run_config.target = aml_remote_compute \n",
    "# runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "script_runconfig.run_config.environment = spark_environment\n",
    "script_runconfig.run_config.framework=\"pyspark\"\n",
    "script_runconfig.run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>test-spark-job-on-amlcompute</td><td>test-spark-job-on-amlcompute_1579846024_06bf69c2</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579846024_06bf69c2?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: test-spark-job-on-amlcompute,\n",
       "Id: test-spark-job-on-amlcompute_1579846024_06bf69c2,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(script_runconfig)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898297be96a1480fb5af0337d7a6cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579846024_06bf69c2?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\", \"run_id\": \"test-spark-job-on-amlcompute_1579846024_06bf69c2\", \"run_properties\": {\"run_id\": \"test-spark-job-on-amlcompute_1579846024_06bf69c2\", \"created_utc\": \"2020-01-24T06:07:06.096572Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"db2c8275-910c-493c-9d20-4d1cb7f74d1f\", \"azureml.git.repository_uri\": \"https://github.com/CESARDELATORRE/poc-spark-aml.git\", \"mlflow.source.git.repoURL\": \"https://github.com/CESARDELATORRE/poc-spark-aml.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"f8730ac0256e1b354b62243fa7afe9b14a69f560\", \"mlflow.source.git.commit\": \"f8730ac0256e1b354b62243fa7afe9b14a69f560\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_ba5559e5971ffa790ebc0193ff8acba1\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-01-24T06:16:34.537449Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=Fda8n4cy5VN9%2FzNQMpANltjWluJE5UPbh2slQOKqI1I%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=op6HmN3FuAwoj6tPhLr52YaijORuekFmTc8%2BTyXL1yk%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=WXimxHjkM3q3P7Qe%2Fr%2FP6WXAYJFxUrwje5CcIGBTlzc%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=WJvFuLED0MzBFEGyFOuJK%2FMU%2Fkuv48Uv5S5a7Q1lpx8%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=KmiV6IPx9vE3va5isZGvDq5xYvAToxQYeZSoQJImrcA%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"azureml-logs/process_info.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=hUrapeTSZrPBu9lTCei8bnRhb3g8RNsDKzdQhJ4Lsmw%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"azureml-logs/process_status.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=kCYVqsfJvijduxlLDYnTNWmptxnUZKv0P2hsmdVl6TY%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=c6cllVXRSfuJaL%2BoSiH8RPSbOwQ0Y%2BksMBRFzsibL%2Fo%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579846024_06bf69c2/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=9p4Q8nhCirWAAlPdi37gBymvBJ1Eyk16FUol5NFk%2FF8%3D&st=2020-01-24T06%3A06%3A46Z&se=2020-01-24T14%3A16%3A46Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/20_image_build_log.txt\"], [\"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"]], \"run_duration\": \"0:09:28\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"bash: /azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/lib/libtinfo.so.5: no version information available (required by bash)\\nbash: /azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/lib/libtinfo.so.5: no version information available (required by bash)\\nTraceback (most recent call last):\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579846024_06bf69c2/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-setup/start_standalone_spark.py\\\", line 34, in <module>\\n    spark_environment_check()\\n  File \\\"/mnt/batch/tasks/shared/LS_root/jobs/cesardl-automl-northcentralus-ws/azureml/test-spark-job-on-amlcompute_1579846024_06bf69c2/mounts/workspaceblobstore/azureml/test-spark-job-on-amlcompute_1579846024_06bf69c2/azureml-setup/spark_env_check.py\\\", line 18, in spark_environment_check\\n    azureml_globals.pyspark_framework_constant))\\nRuntimeError: $SPARK_HOME is not set in the target environment. Please make sure PySpark is installed correctly on the target environment.\\n2020/01/24 06:15:09 mpirun not found, trying job with default values: MPI publisher: open ; version: \\n\\nError occurred: AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"NOTSET\", \"sdk_version\": \"1.0.76\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monitor run\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all metris logged in the run\n",
    "metrics = run.get_metrics()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the generated model\n",
    "model = run.register_model(model_name='iris-spark.model', model_path='outputs/iris-spark.model')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
