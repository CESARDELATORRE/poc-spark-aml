{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure SDK version: 1.0.76\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cesardl-automl-northcentralus-ws\n",
      "automlpmdemo\n",
      "northcentralus\n",
      "102a16c3-37d3-48a8-9237-4c9b1e8e80e0\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        #print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
    "        \n",
    "# Use curated environment for Spark\n",
    "curated_environment = Environment.get(workspace=ws, name=\"AzureML-PySpark-MmlSpark-0.15\")\n",
    "\n",
    "# Save curated environment definition to folder (Two files, one for conda_dependencies.yml and another file for azureml_environment.json)\n",
    "curated_environment.save_to_directory(path=\"./curated_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy based on curated environment\n",
    "spark_environment = curated_environment\n",
    "spark_environment.name = \"Custom-AzureML-PySpark-Environment\"\n",
    "\n",
    "# Create base Environment from Conda specification\n",
    "# spark_environment = Environment.from_conda_specification(name=\"Custom-AzureML-PySpark-Environment\", file_path=\"./curated_environment_definition/conda_dependencies.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ContainerRegistry\n",
    "\n",
    "# Set base Docker Image\n",
    "spark_environment.docker.enabled = True\n",
    "\n",
    "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
    "spark_environment.docker.base_image=\"mcr.microsoft.com/mmlspark/release\" \n",
    "container_registry = ContainerRegistry()\n",
    "container_registry.address = \"mcr.microsoft.com\"\n",
    "# container_registry.username = \"\"   # Use username if using a private Docker Registry like ACR\n",
    "# container_registry.password = \"\"   # Use password if using a private Docker Registry like ACR\n",
    "spark_environment.docker.base_image_registry=container_registry\n",
    "\n",
    "spark_environment.save_to_directory(path=\"./spark_environment_definition\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Name Custom-AzureML-PySpark-Environment\n",
      "packages channels:\n",
      "- conda-forge\n",
      "dependencies:\n",
      "- python=3.6.2\n",
      "- pip:\n",
      "  - azureml-core==1.0.81.1\n",
      "  - azureml-defaults==1.0.81\n",
      "  - azureml-telemetry==1.0.81.1\n",
      "  - azureml-train-restclients-hyperdrive==1.0.81\n",
      "  - azureml-train-core==1.0.81\n",
      "name: azureml_2d6f32a8b5b445b7627fd1ae36599989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register Environment\n",
    "\n",
    "spark_environment.register(ws)\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"Custom\"):\n",
    "        print(\"Environment Name\",env)\n",
    "        print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Experiment\n",
    "\n",
    "from azureml.core import Experiment\n",
    "experiment_name = 'test-spark-job-on-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./project-submit-folder/iris.csv'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create project directory and copy the training script into the project directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './project-submit-folder'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Copy the needed files\n",
    "shutil.copy('spark-job.py', project_folder)\n",
    "shutil.copy('spark-job-simple.py', project_folder)\n",
    "shutil.copy('iris.csv', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing training cluster.\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# Connect or Create a Remote AML compute cluster\n",
    "# Define remote compute target to use\n",
    "# Further docs on Remote Compute Target: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-remote\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cesardl-cpu-clus\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster\n",
    "     # Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 20)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ScriptRunConfig\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "## use pyspark framework\n",
    "# spark_run_config = RunConfiguration(framework=\"pyspark\")\n",
    "## Set compute target to the cluster\n",
    "# spark_run_config.target = aml_remote_compute.name\n",
    "\n",
    "# specify CondaDependencies object to ask system installing numpy\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#add-packages-to-an-environment\n",
    "# cd = CondaDependencies()\n",
    "# cd.add_conda_package('numpy')\n",
    "\n",
    "\n",
    "script_runconfig = ScriptRunConfig(source_directory=project_folder, \n",
    "                                   script=\"spark-job-simple.py\"\n",
    "                                   # run_config = spark_run_config\n",
    "                                  )\n",
    "\n",
    "# Attach compute target to run config\n",
    "script_runconfig.run_config.target = aml_remote_compute \n",
    "# runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "script_runconfig.run_config.environment = spark_environment\n",
    "script_runconfig.run_config.framework=\"pyspark\"\n",
    "script_runconfig.run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>test-spark-job-on-amlcompute</td><td>test-spark-job-on-amlcompute_1579975186_f1de880d</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579975186_f1de880d?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: test-spark-job-on-amlcompute,\n",
       "Id: test-spark-job-on-amlcompute_1579975186_f1de880d,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = experiment.submit(script_runconfig)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7ac1dbbfd94bf6ad9352ed0a09a8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/test-spark-job-on-amlcompute/runs/test-spark-job-on-amlcompute_1579975186_f1de880d?wsid=/subscriptions/102a16c3-37d3-48a8-9237-4c9b1e8e80e0/resourcegroups/automlpmdemo/workspaces/cesardl-automl-northcentralus-ws\", \"run_id\": \"test-spark-job-on-amlcompute_1579975186_f1de880d\", \"run_properties\": {\"run_id\": \"test-spark-job-on-amlcompute_1579975186_f1de880d\", \"created_utc\": \"2020-01-25T17:59:50.329576Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"4f858336-892d-491f-9641-4da4cc735711\", \"azureml.git.repository_uri\": \"https://github.com/CESARDELATORRE/poc-spark-aml.git\", \"mlflow.source.git.repoURL\": \"https://github.com/CESARDELATORRE/poc-spark-aml.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"1439a4e5aab5532f08809258f56c6a12f04e3c32\", \"mlflow.source.git.commit\": \"1439a4e5aab5532f08809258f56c6a12f04e3c32\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_ba5559e5971ffa790ebc0193ff8acba1\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-01-25T18:01:04.07132Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=JZ%2F5m8tKh%2BW1U68oNBR5tULZ%2BLKVz7okFAwgGwowiEg%3D&st=2020-01-25T17%3A51%3A05Z&se=2020-01-26T02%3A01%3A05Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=%2BlLqkcyMtGM1TyIDIE6fXRkgpr7TRQDhKhh%2F5jMu00c%3D&st=2020-01-25T17%3A51%3A05Z&se=2020-01-26T02%3A01%3A05Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=nx7USPUXOwJZBlyfKW6l1jpksuT5DNVF2U0psymAvSE%3D&st=2020-01-25T17%3A51%3A05Z&se=2020-01-26T02%3A01%3A05Z&sp=r\", \"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt?sv=2019-02-02&sr=b&sig=25kQbN3Qhlu6qh8Va3CFRWlT%2Bcdh7IdM5ORYkaJ4g5s%3D&st=2020-01-25T17%3A51%3A06Z&se=2020-01-26T02%3A01%3A06Z&sp=r\", \"azureml-logs/process_info.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=MBu8GQw71bq5pAKs4h49aI4vwKIRQCOH0vC7f8j%2B1xM%3D&st=2020-01-25T17%3A51%3A06Z&se=2020-01-26T02%3A01%3A06Z&sp=r\", \"azureml-logs/process_status.json\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Nd8zZ5bvjKUJr0Dgy%2Bks53vvR4joUZUIegVbmWasAzE%3D&st=2020-01-25T17%3A51%3A06Z&se=2020-01-26T02%3A01%3A06Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=BMp%2B%2BUSdVmL3pzv6z5ueuzSN52J0r2AHM2dQMpIZcoQ%3D&st=2020-01-25T17%3A51%3A06Z&se=2020-01-26T02%3A01%3A06Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://cesardlautomln5648400225.blob.core.windows.net/azureml/ExperimentRun/dcid.test-spark-job-on-amlcompute_1579975186_f1de880d/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=J9l9bwyXzHENy3RH5zCOsbq1z7XJZcTHfyehQ%2BNlK2I%3D&st=2020-01-25T17%3A51%3A06Z&se=2020-01-26T02%3A01%3A06Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_b281321a6953e4a1a8bf07617f11942ddf396ee040e7d06aceec49390496608a_d.txt\"]], \"run_duration\": \"0:01:13\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"bash: /azureml-envs/azureml_a8ad8e485613e21e6e8adc1bfda86b40/lib/libtinfo.so.5: no version information available (required by bash)\\r\\nStarting job release. Current time:2020-01-25T18:00:50.033223\\r\\nLogging experiment finalizing status in history service.\\r\\nStarting the daemon thread to refresh tokens in background for process with pid = 414\\r\\nJob release is complete. Current time:2020-01-25T18:00:51.889653\\r\\n\\nError occurred: AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"NOTSET\", \"sdk_version\": \"1.0.76\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monitor run\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all metris logged in the run\n",
    "metrics = run.get_metrics()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the generated model\n",
    "model = run.register_model(model_name='iris-spark.model', model_path='outputs/iris-spark.model')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
